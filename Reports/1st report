The UNet model has been implemented as described in the V2V paper. As soon as the dataset is available, the reconstruction performance of the model will be observed. This should be important because a bad reconstruction model could extract poor features and overlook correlations between parameters. This could lead to a suboptimal input subset selection.

Still, in contrast to the V2V model, we will not be focusing on the extracted features of the parameters, but we will observe the prediction behavior of different input/output combinations.
To achieve this, the model will be trained applying random dropout to the input layer. The final model trained by this method will be called the probe predictor similar to the original probe denoiser in the original paper. 

The V2V model forces the resolution into 1x512 but The original UNet by Ronneberger downsizes by half as usual. Because we don't focus on the extracted features, there should be no reason to force the resolution into 1D scale. The effect of the bridge part is to be observed further by trying different resolutions.

At the moment our base model does not have any normalization except the one applied on the input at the start. So Batch Normalization inbetween layers might prove useful.
Dropout in hidden layers for regularization is another option. But this could harm our main purpose, because additional dropouts may change the effect of the isolated dropout at the start. For example  This is just an assumption which must be tested.






LRP applied to Meteorological Problems:

