The main question is: Selecting a subset of parameters that give the best prediction accuracy when predicting remaining parameters. 
 
The contribution of each parameter to the prediction is dependent on which other parameters are in the input set.

Because these parameters are closely correlated, observing the individual effect of every parameter and then iteratively selecting the input subset is not the best way to find the optimal input set.

The probe predictor shares the same network architecture as the original UNet reconstructor, but it is trained with random feature dropout at the input layer to "simulate the process of feature selection"

After the training the probe networks is evaluated on different subsets to see compare their impacts on prediction accuracy.

"We then propose a progressive construction process for feature selection, which incrementally builds a feature set based on predictions from the probe denoiser."
"a feature selection algorithm that uses a greedy optimization and a probe denoiser to compute near-optimal sets of features in quadratic time without requiring re-training during the selection process"


Using a larger set of parameters can improve prediction quality, but it incurs additional data loading and IO vs. Data Analysis overhead.
Some parameters are closely correlated and dont need to be read real time as they can be predicted very accurately by others. And some combinations lead to redundancy when predicting, which leads to diminishing returns when adding more parameters to the input subset.

Given a set of parameters, we seek the best input subset consisting of m parameters, that predicts the remaining n-m parameters most accurately. 
Also a tradeoff between the number of parameters and the prediction quality is to be examined.

A brute force approach where a network is trained for each subset is computationally very expensive.

"A more efficient solution with low approximation error requires to train only one network.
We use this network as an oracle for predicting the impact of parameter combinations on the prediction accuracy"
"Additionally, we propose a greedy feature set selection algorithm that only requires testing at most O(|P|^2) subsets with the probe denoiser, avoiding the combinatorial complexity at the cost of some approximation error."
"It is possible to evaluate the impact on the denoising error of a feature set S using a denoiser trained with the full feature set P, i.e., gP , and manually turning off other features q ∈ P\S during inference."
"However, inputs [I, S] with smaller feature subsets will inevitably be out-of-distribution for gP. This undermines the reliability of the measured denoising error between different feature subsets"
"To mitigate the out-of-distribution issue, we use random feature dropout during the training of a denoiser that we will use for feature-set probing."
"The trained probe denoiser can more reliably predict the denoising quality obtained by using smaller feature subsets as we train it on feature sets with missing features."
"Despite the difference in how a probe denoiser is trained compared to a regular denoiser, we later demonstrate that its denoising error for a specific feature set correlates quite well with the denoising error of a specialized denoiser trained only on that set."
"On the other hand, the error of a denoiser trained without dropout does not have a good correlation with the final denoising error, especially on smaller feature subsets."
"The candidate feature set of size i yields the minimum average denoising error ℓ of the probe denoiser g among all feature sets of size i."
"finding the global optimum of the optimization problem(set of size i with minimum error) requires evaluating the loss ℓ on all subsets, making it intractable even for moderately sized feature sets."
"Therefore, we propose a greedy solution to construct the elements of P̄ in an incremental fashion"
"Always selecting the one feature that improves denoising quality the most, as measured by the probe denoiser, when adding it to the set of already selected features."

1- Start from an empty set of features S_0 = { }
2- For all remaining features q e P\S_i compute l(S_i U {q})
3- Set S_i+1 = S_i U {q_i*} where q_i* = argmin_q(l)

After the candidate feature sets for each size are established, we can select the feature set achieving a desired tradeoff between cost and quality.
Tradeoff with two constraining parameters: the maximum number of affordable features and the minimal acceptable denoising quality gain, relative to the gain using the best performing candidate set compared to using no features.








